{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0491d240-77c2-4587-a3e9-028cf68827e6",
   "metadata": {},
   "source": [
    "# Fig 2. Benchmark of host identification performance\n",
    "\n",
    "####  Dependencies:\n",
    "    - kraken2==2.1.3\n",
    "    - metamaps==0.1\n",
    "    - megan==6.25.9\n",
    "    - argo==0.1.0\n",
    "    - pandas\n",
    "    - taxonkit\n",
    "    - biopython\n",
    "\n",
    "#### Inputs:\n",
    "    - fig2/data/*-arg.fa (ARG-containing reads of HQ and LQ)\n",
    "    - fig2/data/reference (reference genomes and metadata of the top 25 pathgens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a33649-d584-43a3-891e-8701e150c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# mkdir -p fig2/tmp/kraken fig2/tmp/centrifuger fig2/tmp/metamaps/ fig2/tmp/megan fig2/tmp/minimap\n",
    "# for file in fig2/data/*.fa\n",
    "# do\n",
    "#     filename=${file%.fa}\n",
    "#     filename=${filename##*/}\n",
    "\n",
    "#     ## kraken\n",
    "#     kraken2 $file --output fig2/tmp/kraken/$filename.output --db db/kraken --threads 48\n",
    "\n",
    "#     ## centrifuger\n",
    "#     centrifuger -u $file -x db/centrifuger -t 48 > fig2/tmp/centrifuger/$filename.tsv\n",
    "\n",
    "#     ## metamaps\n",
    "#     metamaps mapDirectly --all -r db/metamaps/DB.fa -q $file -o fig2/tmp/metamaps/$filename -t 48 --maxmemory 20\n",
    "#     metamaps classify --mappings fig2/tmp/metamaps/$filename --DB db/metamaps -t 48\n",
    "\n",
    "#     ## megan\n",
    "#     minimap2 -ax map-ont --split-prefix=$filename --sam-hit-only -I 8G -t 48 db/refseq.fna.gz $file > fig2/tmp/megan/$filename.sam\n",
    "#     sam2rma -i fig2/tmp/megan/$filename.sam -mdb db/megan -r $file -o fig2/tmp/megan/$filename.rma -c false -lg -alg longReads -ram readCount -t 48\n",
    "#     rma2info -i fig2/tmp/megan/$filename.rma -o fig2/tmp/megan/$filename.txt -r2c Taxonomy\n",
    "\n",
    "#     ## minimap\n",
    "#     minimap2 -cx map-ont -t 48 db/refseq.fna.gz $file > fig2/tmp/minimap/$filename.paf\n",
    "#     minimap2 -x ava-ont -t 48 $file $file > fig2/tmp/minimap/$filename-self.paf\n",
    "# done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68c6877-4a8a-487a-98df-e74e118cff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenxi/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from Bio import SeqIO\n",
    "from argo.utils import *\n",
    "\n",
    "## create an accession to species (ground truth) mapping\n",
    "metadata = pd.read_table('fig2/data/reference/data_summary.tsv')\n",
    "metadata['species'] = metadata['Organism Scientific Name'].str.split(' ').str.get(0) + ' ' + metadata['Organism Scientific Name'].str.split(' ').str.get(1)\n",
    "\n",
    "row = []\n",
    "for file in glob.glob('fig2/data/reference/*/*.fna'):\n",
    "    with open(file) as handle:\n",
    "        for record in SeqIO.parse(handle, 'fasta'):\n",
    "            row.append([record.id, 'GCF_' + file.split('/')[-1].split('_')[1]])\n",
    "\n",
    "accession2species = pd.merge(pd.DataFrame(row, columns = ['accession', 'Assembly Accession']), metadata).set_index('accession').species.to_dict()\n",
    "\n",
    "## record all read ids\n",
    "ids = defaultdict(list)\n",
    "for file in glob.glob('fig2/data/*.fa'):\n",
    "    filename = file.split('.fa')[0].split('/')[-1]\n",
    "    with open(file) as handle:\n",
    "        for record in SeqIO.parse(handle, 'fasta'):\n",
    "            ids[filename].append(record.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e02fef-44a3-4843-8e90-3b118cdef18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxonomy(taxid):\n",
    "    output = subprocess.run([\n",
    "        'taxonkit', 'reformat',\n",
    "        '--taxid-field', '1',\n",
    "        '--show-lineage-taxids',\n",
    "        '--fill-miss-rank',\n",
    "        '--miss-taxid-repl', '0',\n",
    "        '--miss-rank-repl', 'unclassified',\n",
    "        '--trim',\n",
    "        '-f', '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}'],\n",
    "        input='\\n'.join(taxid)+'\\n', text=True, capture_output=True, check=True)\n",
    "\n",
    "    taxonomy = {}\n",
    "    for line in output.stdout.rstrip().split('\\n'):\n",
    "        ls = line.rstrip().split('\\t')\n",
    "        taxonomy[int(ls[0])] = ';'.join([ls[i+7] + '|' + ls[i] for i in range(1, len(ls)-7)])\n",
    "\n",
    "    return taxonomy\n",
    "\n",
    "def parser(df, method, file):\n",
    "    species2unclassified = df[df.est=='unclassified'].groupby('species').size().to_dict()\n",
    "    species2misclassified = df[df.est!=df.species].groupby('species').size().to_dict()\n",
    "    species2tp = df[(df.est!='unclassified') & (df.est!=df.species) & (df.est.isin(set(df.species)))].groupby('species').size().to_dict()\n",
    "    species2fp = df[(df.est!='unclassified') & (df.est!=df.species) & (~df.est.isin(set(df.species)))].groupby('species').size().to_dict()\n",
    "    sp = df.groupby('species', as_index=False).size()\n",
    "    \n",
    "    sp['unclassified'] = sp.species.map(species2unclassified).fillna(0)\n",
    "    sp['misclassified'] = sp.species.map(species2misclassified).fillna(0)\n",
    "    sp = sp.assign(method=method, file=file.split('-')[0])\n",
    "\n",
    "    st = pd.DataFrame([[\n",
    "        sum(species2unclassified.values()) / len(df),\n",
    "        sum(species2misclassified.values()) / len(df),\n",
    "        sum(species2tp.values()) / len(df),\n",
    "        sum(species2fp.values()) / len(df)\n",
    "    ]], columns = ['unclassified', 'misclassified', 'tp', 'fp']).assign(method=method, file=file.split('-')[0])\n",
    "\n",
    "    species.append(sp)\n",
    "    statistics.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91078498-ba23-435c-9c66-7058fa553565",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['HQ-arg', 'LQ-arg']\n",
    "species, statistics = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5252c6-5266-4c2b-b70b-6891a8d1dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## kraken\n",
    "for filename in files:\n",
    "    row = []\n",
    "    with open(f'fig2/tmp/kraken/{filename}.output') as f:\n",
    "        for line in f:\n",
    "            ls = line.rstrip().split('\\t')\n",
    "            row.append([ls[1], int(ls[2]), int(ls[3])])\n",
    "            \n",
    "    df = pd.merge(pd.DataFrame(ids[filename]), pd.DataFrame(row), how='left').fillna(0)\n",
    "    df['est'] = df[1].map(get_taxonomy(df[1].astype(str).unique())).str.split('|').str.get(-1)\n",
    "    df['species'] = df[0].str.split('-').str.get(0).map(accession2species)\n",
    "\n",
    "    parser(df, 'Kraken2', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283fb51d-b21c-43f0-ac38-b21cde57a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## centrifuger\n",
    "for filename in files:\n",
    "    df = pd.read_table(f'fig2/tmp/centrifuger/{filename}.tsv')\n",
    "    df = pd.merge(pd.DataFrame(ids[filename]), df, how='left', left_on=0, right_on='readID').fillna(0)\n",
    "    df['est'] = df['taxID'].map(get_taxonomy(df['taxID'].astype(str).unique())).str.split('|').str.get(-1)\n",
    "    df['species'] = df[0].str.split('-').str.get(0).map(accession2species)\n",
    "\n",
    "    parser(df, 'Centrifuger', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32d21f3-18de-4591-a4d4-1d30628e24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## metamaps\n",
    "for filename in files:\n",
    "    df = pd.read_table(f'fig2/tmp/metamaps/{filename}.EM.reads2Taxon', header=None)\n",
    "    df = pd.merge(pd.DataFrame(ids[filename]), df, how='left').fillna(0)\n",
    "    df[1] = df[1].astype(int)\n",
    "    df['est'] = df[1].map(get_taxonomy(df[1].astype(str).unique())).str.split('|').str.get(-1)\n",
    "    df['species'] = df[0].str.split('-').str.get(0).map(accession2species)\n",
    "\n",
    "    parser(df, 'MetaMaps', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7348c117-ce74-41c3-82da-c054be8ee8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## megan\n",
    "for filename in files:\n",
    "    df = pd.read_table(f'fig2/tmp/megan/{filename}.txt', header=None)\n",
    "    df = pd.merge(pd.DataFrame(ids[filename]), df, how='left').fillna(0)\n",
    "    df[1] = df[1].astype(int)\n",
    "    df['est'] = df[1].map(get_taxonomy(df[1].astype(str).unique())).str.split('|').str.get(-1)\n",
    "    df['species'] = df[0].str.split('-').str.get(0).map(accession2species)\n",
    "\n",
    "    parser(df, 'MEGAN-LR', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d1f84b6-72cb-47d1-aedd-e8331a45f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "## minimap2+bh\n",
    "assembly2species = pd.read_table('db/refseq.assembly2species.tsv').set_index('assembly').species.to_dict()\n",
    "accession2assembly = pd.read_table('db/refseq.accession2assembly.tsv').set_index('accession').assembly.to_dict()\n",
    "\n",
    "for filename in files:\n",
    "    row = []\n",
    "    with open(f'fig2/tmp/minimap/{filename}.paf') as f:\n",
    "        for line in f:\n",
    "            ls = line.rstrip().split()\n",
    "            row.append([ls[0], assembly2species.get(accession2assembly.get(ls[5])), int(ls[14].split('AS:i:')[-1])])\n",
    "\n",
    "    df = pd.DataFrame(row).sort_values(2, ascending=False)\n",
    "    df = df.groupby(0, as_index=False).first()\n",
    "    df = pd.merge(pd.DataFrame(ids[filename]), df, how='left').fillna(0)\n",
    "\n",
    "    df['est'] = df[1].str.split('|').str.get(-1)\n",
    "    df['species'] = df[0].str.split('-').str.get(0).map(accession2species)\n",
    "\n",
    "    parser(df, 'minimap2+BH', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80098fbf-628c-457e-be3d-cf20a59f4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## minimap2+em and minimap2+bh\n",
    "assembly2species = pd.read_table('db/refseq.assembly2species.tsv').set_index('assembly').species.to_dict()\n",
    "accession2assembly = pd.read_table('db/refseq.accession2assembly.tsv').set_index('accession').assembly.to_dict()\n",
    "\n",
    "for filename in files:\n",
    "    alignments = []\n",
    "    scores = defaultdict(lambda: defaultdict(lambda: {'AS': 0, 'DE': 0, 'ID': 0}))\n",
    "\n",
    "    with open(f'fig2/tmp/minimap/{filename}.paf') as f:\n",
    "        for line in f:\n",
    "            ls = line.rstrip().split('\\t')\n",
    "            qstart, qend, qseqid, sseqid = int(ls[2]), int(ls[3]), ls[0], ls[5]\n",
    "            lineage = assembly2species.get(accession2assembly.get(sseqid)).split('|')[-1]\n",
    "    \n",
    "            AS_MAX, AS = scores[qseqid][lineage].get('AS', 0), int(ls[14].split('AS:i:')[-1])\n",
    "            DE_MAX, DE = scores[qseqid][lineage].get('DE', 0), 1 - float((ls[19] if ls[16] in {'tp:A:S', 'tp:A:i'} else ls[20]).split('de:f:')[-1])\n",
    "            ID_MAX, ID = scores[qseqid][lineage].get('ID', 0), int(ls[9]) / int(ls[10])\n",
    "    \n",
    "            ## filter out non-overlapping alignments\n",
    "            if AS > AS_MAX or DE > DE_MAX or ID > ID_MAX:\n",
    "                scores[qseqid][lineage]['AS'] = max(AS_MAX, AS)\n",
    "                scores[qseqid][lineage]['DE'] = max(DE_MAX, DE)\n",
    "                scores[qseqid][lineage]['ID'] = max(ID_MAX, ID)\n",
    "                alignments.append([qseqid, sseqid, AS, DE, ID, lineage])\n",
    "\n",
    "    ## filter out low-score alignments\n",
    "    duplicates = set()\n",
    "    max_scores = defaultdict(lambda: {'AS': 0, 'DE': 0, 'ID': 0})\n",
    "    \n",
    "    for alignment in alignments:\n",
    "        max_scores[alignment[0]]['AS'] = max(max_scores[alignment[0]]['AS'], alignment[2])\n",
    "        max_scores[alignment[0]]['DE'] = max(max_scores[alignment[0]]['DE'], alignment[3])\n",
    "        max_scores[alignment[0]]['ID'] = max(max_scores[alignment[0]]['ID'], alignment[4])\n",
    "    \n",
    "    sa = []\n",
    "    for alignment in sorted(alignments, key=lambda alignment: (alignment[0], alignment[2], alignment[3], alignment[4]), reverse=True):\n",
    "        if (\n",
    "            max(alignment[2] / 0.995, alignment[2] + 50) > max_scores[alignment[0]]['AS']\n",
    "        ):\n",
    "            if (alignment[0], alignment[-1]) not in duplicates:\n",
    "                sa.append(alignment)\n",
    "                duplicates.add((alignment[0], alignment[-1]))\n",
    "\n",
    "    ## EM\n",
    "    max_iteration = 1000\n",
    "    epsilon = 1e-10\n",
    "    em = {}\n",
    "    ## create a matrix then fill\n",
    "    qseqids, lineages = np.unique([alignment[0] for alignment in sa]), np.unique([alignment[-1] for alignment in sa])\n",
    "    qseqid2index = {qseqid: index for index, qseqid in enumerate(qseqids)}\n",
    "    lineage2index = {lineage: index for index, lineage in enumerate(lineages)}\n",
    "    \n",
    "    rows = [qseqid2index[alignment[0]] for alignment in sa]\n",
    "    cols = [lineage2index[alignment[-1]] for alignment in sa]\n",
    "    matrix = csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(len(qseqids), len(lineages)), dtype=int)\n",
    "    \n",
    "    ## run EM using the count matrix as input\n",
    "    n_reads, n_mappings = matrix.shape\n",
    "    \n",
    "    ## init\n",
    "    p_mappings = np.ones((1, n_mappings)) / n_mappings\n",
    "    p_mappings_hist = p_mappings.copy()\n",
    "    \n",
    "    iteration = 0\n",
    "    while iteration < max_iteration:\n",
    "        iteration += 1\n",
    "    \n",
    "        ## e-step\n",
    "        p_reads = matrix.multiply(p_mappings) / matrix.dot(p_mappings.T)\n",
    "    \n",
    "        ## m-step\n",
    "        p_mappings = np.sum(p_reads, axis=0) / n_reads\n",
    "    \n",
    "        ## check convergence\n",
    "        if np.sum(np.abs(p_mappings - p_mappings_hist)) < epsilon:\n",
    "            break\n",
    "    \n",
    "        ## update hist\n",
    "        np.copyto(p_mappings_hist, p_mappings)\n",
    "    \n",
    "    ## return assignments\n",
    "    assignments = []\n",
    "    for p_read in p_reads.tocsr():\n",
    "        p_read = p_read.toarray().squeeze()\n",
    "        assignments.append(np.where(p_read == p_read.max())[0].tolist())\n",
    "    \n",
    "    ties = defaultdict(set)\n",
    "    for qseqid, lineage in enumerate(assignments):\n",
    "        if len(assignment := lineages[lineage]) > 1:\n",
    "            ties[tuple(assignment)].add(qseqids[qseqid])\n",
    "        else:\n",
    "            em[qseqids[qseqid]] = assignment[0]\n",
    "    \n",
    "    ## resolve ties for equal probability cases using AS, MS and ID\n",
    "    if ties:\n",
    "        qset = set.union(*(set(qseqid) for qseqid in ties.values()))\n",
    "        alignments = [alignment for alignment in sa if alignment[0] in qset]\n",
    "    \n",
    "        for lineages, qseqids in ties.items():\n",
    "            targets = [alignment for alignment in alignments if alignment[0] in qseqids and alignment[-1] in lineages]\n",
    "    \n",
    "            scores = defaultdict(lambda: defaultdict(list))\n",
    "            for target in targets:\n",
    "                scores[target[-1]]['AS'].append(target[2])\n",
    "                scores[target[-1]]['DE'].append(target[3])\n",
    "                scores[target[-1]]['ID'].append(target[4])\n",
    "    \n",
    "            ## if all the same, choose the one with known species name\n",
    "            target = sorted([\n",
    "                [\n",
    "                    np.mean(score['AS']),\n",
    "                    np.mean(score['DE']),\n",
    "                    np.mean(score['ID']),\n",
    "                    not bool(re.search(r' sp\\.$| sp\\. | sp[0-9]+', lineage.split(';')[-1])),\n",
    "                    lineage\n",
    "                ] for lineage, score in scores.items()\n",
    "            ], reverse=True)[0][-1]\n",
    "    \n",
    "            for qseqid in qseqids:\n",
    "                em[qseqid] = target\n",
    "\n",
    "    df = pd.DataFrame(ids[filename])\n",
    "    df['est'] = df[0].map(em)\n",
    "    df['species'] = df[0].str.split('-').str.get(0).map(accession2species)\n",
    "\n",
    "    parser(df, 'minimap2+EM', filename)\n",
    "\n",
    "    ## SC\n",
    "    overlaps = filter_overlap(file=f\"fig2/tmp/minimap/{filename}-self.paf\")\n",
    "    DV = np.median([overlap[-1] for overlap in overlaps])\n",
    "    overlaps = [overlap for overlap in overlaps if overlap[-1] <= 2.5 * DV]\n",
    "\n",
    "    nodes = np.unique(ids[filename])\n",
    "    node2index = {node: index for index, node in enumerate(nodes)}\n",
    "    identities = defaultdict(lambda: 0)\n",
    "    \n",
    "    for overlap in overlaps:\n",
    "        if (row := node2index.get(overlap[0])) and (col := node2index.get(overlap[1])):\n",
    "            identities[(row, col)] = identities[(col, row)] = max(1 - overlap[-1], identities.get((row, col), 0), identities.get((col, row), 0))\n",
    "    \n",
    "    matrix = dok_matrix((len(nodes), len(nodes)))\n",
    "    rows, cols = zip(*identities.keys())\n",
    "    matrix[rows, cols] = list(identities.values())\n",
    "    clusters = mcl(matrix, max_iterations=1000, inflation=2, expansion=2)\n",
    "    index2label = {index: label for label, indexes in enumerate(clusters) for index in indexes}\n",
    "    labels = np.array([index2label.get(index) for index in range(len(nodes))])\n",
    "    clusters = [nodes[labels==label] for label in np.unique(labels)[::-1]]\n",
    "\n",
    "    sc = dict()\n",
    "    for index, cluster in enumerate(clusters):\n",
    "        elements = set(cluster)\n",
    "    \n",
    "        ## append scores of covered reads\n",
    "        subsets = defaultdict(set)\n",
    "        scores = defaultdict(lambda: defaultdict(dict))\n",
    "        alignments = [alignment for alignment in sa if alignment[0] in elements]\n",
    "        for alignment in alignments:\n",
    "            subsets[alignment[-1]].add(alignment[0])\n",
    "            scores[alignment[-1]][alignment[0]] = alignment[2]\n",
    "    \n",
    "        ## get covers\n",
    "        covers = set_cover(elements, subsets, scores)\n",
    "        if len(covers) >= 2:\n",
    "            qseqid2lineage = dict()\n",
    "            score = defaultdict(lambda: 0)\n",
    "            for qseqid in set.union(*[subsets[cover] for cover in covers]):\n",
    "                for cover in covers:\n",
    "                    AS = scores[cover].get(qseqid, 0)\n",
    "                    if AS > score.get(qseqid, 0):\n",
    "                        qseqid2lineage[qseqid] = cover\n",
    "                        score[qseqid] = AS\n",
    "            sc.update(qseqid2lineage)\n",
    "    \n",
    "        if len(covers)==1:\n",
    "            sc.update({qseqid: covers[0] for qseqid in subsets[covers[0]]})\n",
    "\n",
    "    df = pd.DataFrame(ids[filename])\n",
    "    df['est'] = df[0].map(sc)\n",
    "    df['species'] = df[0].str.split('-').str.get(0).map(accession2species)\n",
    "\n",
    "    parser(df, 'minimap2+RO', filename)\n",
    "\n",
    "    ## record cluster size\n",
    "    r = []\n",
    "    for cluster in clusters:\n",
    "        tmp = [assembly2species.get(accession2assembly.get(x.split('-')[0])) for x in cluster]\n",
    "        r.append([Counter(tmp).most_common(1)[0][1] / len(tmp), len(tmp)])\n",
    "    pd.DataFrame(r, columns = ['purity', 'size']).to_csv(f'fig2/{filename}.size.tsv', index=False, sep='\\t')\n",
    "\n",
    "    ## record cluster scov\n",
    "    arg = pd.read_table(f'sarg/{filename}.sarg.tsv', header=None, names=['qseqid', 'qlen', 'sseqid', 'sstart', 'send', 'slen'])\n",
    "    arg['scov'] = (arg['send'] - arg['sstart']) / arg.slen\n",
    "    sscovs = defaultdict(lambda: defaultdict(set))\n",
    "    for _, i in arg.iterrows():\n",
    "        sscovs[i.qseqid][(i.sseqid, i.slen)].add((i.sstart, i.send))\n",
    "    \n",
    "    discarded_hits = defaultdict(set)\n",
    "    r = []\n",
    "    for elements in clusters:\n",
    "        scovs = dict()\n",
    "        merged_scovs = defaultdict(set)\n",
    "        for scov in [sscovs.get(element) for element in elements]:\n",
    "            for sseqid, coordinates in scov.items():\n",
    "                merged_scovs[sseqid].update(coordinates)\n",
    "    \n",
    "        for sseqid, coordinates in merged_scovs.items():\n",
    "            coordinates = list(sorted(coordinates))\n",
    "            merged_coordinates = [list(coordinates[0])] \n",
    "    \n",
    "            for coordinate in coordinates[1:]:\n",
    "                if coordinate[0] <= merged_coordinates[-1][1]:\n",
    "                    merged_coordinates[-1][1] = max(merged_coordinates[-1][1], coordinate[1])\n",
    "                else:\n",
    "                    merged_coordinates.append(list(coordinate))\n",
    "    \n",
    "            for element in elements:\n",
    "                r.append([sseqid[0], element, sum([coordinate[1] - coordinate[0] for coordinate in merged_coordinates]) / sseqid[1]])\n",
    "    \n",
    "    pd.merge(\n",
    "        pd.DataFrame(r, columns = ['sseqid', 'qseqid', 'scov-c']).assign(mode = 'clustered'),\n",
    "        arg.assign(mode = 'raw')[['qseqid', 'sseqid', 'scov', 'mode']], \n",
    "        on = ['qseqid', 'sseqid'], how='right').to_csv(f'fig2/{filename}.scov.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb6f6d4-d37e-4eca-99ba-945460b340fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(species).to_csv('fig2/species.tsv', sep='\\t', index=False)\n",
    "pd.concat(statistics).to_csv('fig2/statistics.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
