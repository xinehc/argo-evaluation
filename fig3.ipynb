{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca26932-bb6e-467a-b79c-27379c9d7c1f",
   "metadata": {},
   "source": [
    "# Fig 3. Comparison between ARG profiling methods\n",
    "\n",
    "####  Dependencies:\n",
    "    - flye==2.9.3\n",
    "    - semibin==2.1.0\n",
    "    - kraken2==2.1.3\n",
    "    - gtdbtk==2.4.0\n",
    "    - argo==0.1.0\n",
    "    - pandas\n",
    "    - tqdm\n",
    "    - taxonkit\n",
    "\n",
    "#### Inputs:\n",
    "    - fig3/data/*_*x.fa (downsampled 1â€“32x HQ and LQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802fee0c-c3a9-4273-85c9-8c07f2679179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# mkdir -p fig3/tmp\n",
    "# for file in fig3/data/*.fa\n",
    "# do\n",
    "#     filename=${file##*/}\n",
    "#     filename=${filename%%.*}\n",
    "    \n",
    "#     ## metaflye\n",
    "#     flye --nano-raw $file \\\n",
    "#         --out-dir fig3/tmp/$filename \\\n",
    "#         --meta \\\n",
    "#         --threads 32\n",
    "    \n",
    "#     ## semibin\n",
    "#     minimap2 -ax map-ont -t 32 fig3/tmp/$filename/assembly.fasta $file | samtools sort > fig3/tmp/$filename/assembly.bam\n",
    "#     SemiBin2 single_easy_bin \\\n",
    "#         --sequencing-type=long_read \\\n",
    "#         --environment global \\\n",
    "#         -i fig3/tmp/$filename/assembly.fasta \\\n",
    "#         -o fig3/tmp/$filename \\\n",
    "#         -t 32 \\\n",
    "#         -b fig3/tmp/$filename/assembly.bam \\\n",
    "#         --engine cpu\n",
    "\n",
    "#     ## get ARG types/subtypes of contigs\n",
    "#     diamond blastx \\\n",
    "#         -q fig3/tmp/$filename/assembly.fasta --threads 32 \\\n",
    "#         -d db/argo/sarg.dmnd -o fig3/tmp/$filename/sarg.tsv \\\n",
    "#         --id 90 --subject-cover 90 -e 1e-15 \\\n",
    "#         -F15 --range-culling --range-cover 25 \\\n",
    "#         --max-hsps 0 --max-target-seqs 25 \\\n",
    "#         --outfmt 6 qseqid sseqid pident length qlen qstart qend slen sstart send evalue bitscore --quiet\n",
    "\n",
    "#     ## kraken\n",
    "#     kraken2 fig3/tmp/$filename/assembly.fasta \\\n",
    "#         --db db/kraken-pluspf \\\n",
    "#         --threads 32 \\\n",
    "#         --output fig3/tmp/$filename/assembly.output --use-names \\\n",
    "#         --report fig3/tmp/$filename/assembly.report\n",
    "\n",
    "#     ## gtdbtk\n",
    "#     gtdbtk classify_wf \\\n",
    "#         --genome_dir fig3/tmp/$filename/output_bins/ \\\n",
    "#         --cpus 32 \\\n",
    "#         --out_dir fig3/tmp/$filename/gtdbtk \\\n",
    "#         --extension fa.gz \\\n",
    "#         --mash_db db/gtdbtk/release220/mash/\n",
    "\n",
    "#     ## argo\n",
    "#     argo $file -d db/argo -o fig3/tmp/$filename -t 32 --plasmid\n",
    "# done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e1b8e8-29eb-4dea-a009-ba5d97b8073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenxi/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "from melon.utils import *\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cityblock, euclidean\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_diamond(file):\n",
    "    qcoords = defaultdict(set)\n",
    "    hits = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            ls = line.rstrip().split('\\t')\n",
    "            qseqid, sseqid = ls[0], ls[1]\n",
    "            qstart, qend = sort_coordinate(int(ls[5]), int(ls[6]))\n",
    "            sstart, send = sort_coordinate(int(ls[8]), int(ls[9]))\n",
    "            slen = int(ls[7])\n",
    "            qcovhsp, scovhsp = float(ls[-2]), float(ls[-1])\n",
    "            if (\n",
    "                qseqid not in qcoords or\n",
    "                all(compute_overlap((qstart, qend, *qcoord), max) < 0.25 for qcoord in qcoords[qseqid])\n",
    "            ):\n",
    "                qcoords[qseqid].add((qstart, qend))\n",
    "    \n",
    "                ## append qseqid and coordinates for back-tracing\n",
    "                hits.append([qseqid, sseqid, qstart, qend, float(ls[2])])\n",
    "    return(hits)\n",
    "\n",
    "def get_taxonomy(taxid):\n",
    "    output = subprocess.run([\n",
    "        'taxonkit', 'reformat',\n",
    "        '--taxid-field', '1',\n",
    "        '--show-lineage-taxids',\n",
    "        '--fill-miss-rank',\n",
    "        '--miss-taxid-repl', '0',\n",
    "        '--miss-rank-repl', 'unclassified',\n",
    "        '--trim',\n",
    "        '-f', '{k}\\t{p}\\t{c}\\t{o}\\t{f}\\t{g}\\t{s}'],\n",
    "        input='\\n'.join(taxid)+'\\n', text=True, capture_output=True, check=True)\n",
    "\n",
    "    taxonomy = {}\n",
    "    for line in output.stdout.rstrip().split('\\n'):\n",
    "        ls = line.rstrip().split('\\t')\n",
    "        taxonomy[int(ls[0])] = ';'.join([ls[i+7] + '|' + ls[i] for i in range(1, len(ls)-7)])\n",
    "\n",
    "    return taxonomy\n",
    "\n",
    "def pr(df):\n",
    "    df = df[df.species!='unclassified']\n",
    "    TP = len(df[(df['est']!=0) & (df['cnt']!=0)])\n",
    "    TPFP = len(df[df['est']!=0])\n",
    "    TPFN = len(df[df['cnt']!=0])\n",
    "    precision, recall = TP/TPFP, TP/TPFN\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5a9493-08d0-476f-aefb-418e8173d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_kraken(filename):\n",
    "    ctg = []\n",
    "    with open(f'fig3/tmp/{filename}/assembly.output') as f:\n",
    "        for line in f:\n",
    "            ls = line.split()\n",
    "            contig = line.split('\\t')[1]\n",
    "            taxid = line.split('(taxid ')[-1].split(')')[0]\n",
    "            ctg.append([contig, int(taxid)])\n",
    "\n",
    "    ctg = pd.DataFrame(ctg, columns = ['contig', 'taxid'])\n",
    "    ctg['taxonomy'] = ctg['taxid'].map(get_taxonomy(ctg['taxid'].astype(str).unique()))\n",
    "    ctg['species'] = ctg.taxonomy.str.split('|').str.get(-1)\n",
    "\n",
    "    c2kraken = ctg.set_index('contig').species.to_dict()\n",
    "\n",
    "    sarg = pd.DataFrame(parse_diamond(f'fig3/tmp/{filename}/sarg.tsv'), columns = ['contig', 'sarg', 'qstart', 'qend', 'pident'])\n",
    "    sarg['type'] = sarg['sarg'].str.split('|').str.get(1).str.split('@').str.get(0)\n",
    "    sarg['subtype'] = sarg['sarg'].str.split('|').str.get(2)\n",
    "    sarg['species'] = sarg.contig.map(c2kraken)\n",
    "    \n",
    "    return (\n",
    "        sarg.groupby(['species', 'type'], as_index=False).size().rename(columns={'size': 'est'}), \n",
    "        sarg.groupby(['species', 'subtype'], as_index=False).size().rename(columns={'size': 'est'})\n",
    "    )\n",
    "\n",
    "def parse_gtdb(filename):\n",
    "    c2b = pd.read_table(f'fig3/tmp/{filename}/contig_bins.tsv').set_index('contig').bin.to_dict()\n",
    "    gtdb = pd.read_table(f'fig3/tmp/{filename}/gtdbtk/gtdbtk.bac120.summary.tsv')\n",
    "    gtdb['bin'] = gtdb.user_genome.str.split('_').str.get(-1).astype(int)\n",
    "    gtdb['species'] = gtdb.classification.str.split('s__').str.get(-1)\n",
    "    b2gtdb = gtdb.set_index('bin').species.to_dict()\n",
    "    c2gtdb = {x: b2gtdb.get(y) for x,y in c2b.items()}\n",
    "    \n",
    "    sarg = pd.DataFrame(parse_diamond(f'fig3/tmp/{filename}/sarg.tsv'), columns = ['contig', 'sarg', 'qstart', 'qend', 'pident'])\n",
    "    sarg['type'] = sarg['sarg'].str.split('|').str.get(1).str.split('@').str.get(0)\n",
    "    sarg['subtype'] = sarg['sarg'].str.split('|').str.get(2)\n",
    "    sarg['species'] = sarg.contig.map(c2gtdb)\n",
    "\n",
    "    sarg.species = sarg.species.fillna('unclassified').apply(lambda x: re.sub('_[A-Z]','',x))\n",
    "    sarg.species = sarg.species.apply(lambda x: {\n",
    "        'Bacillus thuringiensis': 'Bacillus cereus',\n",
    "        '': 'unclassified',\n",
    "        'Unclassified Bacteria': 'unclassified'\n",
    "    }.get(x,x))\n",
    "    \n",
    "    return (\n",
    "        sarg.groupby(['species', 'type'], as_index=False).size().rename(columns={'size': 'est'}), \n",
    "        sarg.groupby(['species', 'subtype'], as_index=False).size().rename(columns={'size': 'est'})\n",
    "    )\n",
    "\n",
    "def parse_argo(filename):\n",
    "    sarg = pd.read_table(f'fig3/tmp/{filename}/{filename}.sarg.tsv')\n",
    "    sarg['species'] = sarg['lineage'].str.split(';').str.get(-1)\n",
    "    sarg.species = sarg.species.apply(lambda x: re.sub('_[A-Z]','',x))\n",
    "    sarg.species = sarg.species.apply(lambda x: {'Bacillus thuringiensis': 'Bacillus cereus'}.get(x,x))\n",
    "\n",
    "    return (\n",
    "        sarg.groupby(['species', 'type'], as_index=False)['abundance'].sum().rename(columns={'abundance': 'est'}), \n",
    "        sarg.groupby(['species', 'subtype'], as_index=False)['abundance'].sum().rename(columns={'abundance': 'est'}), \n",
    "    )\n",
    "\n",
    "def parse_real():\n",
    "    sarg = pd.read_table('sarg/pathogen.sarg.tsv')\n",
    "    return (\n",
    "        sarg.groupby(['species', 'type'], as_index=False)['cnt'].sum(),\n",
    "        sarg.groupby(['species', 'subtype'], as_index=False)['cnt'].sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8fe759-8395-4efe-a4e8-8fe59c5a22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tval = []\n",
    "sval = []\n",
    "\n",
    "tstat = []\n",
    "sstat = []\n",
    "for filename in tqdm(glob.glob('fig3/data/*.fa')):\n",
    "    filename = filename.split('/')[-1].split('.')[0]\n",
    "    sample = filename.split('_')[0]\n",
    "    coverage = int(filename.split('_')[1].split('x')[0])\n",
    "\n",
    "    real_type, real_subtype = parse_real()\n",
    "    read_type, read_subtype = parse_argo(filename)\n",
    "    assembly_type, assembly_subtype = parse_kraken(filename)\n",
    "    binning_type, binning_subtype = parse_gtdb(filename)\n",
    "\n",
    "    a = pd.merge(real_type, read_type, how='outer').assign(method='read-based').fillna(0)\n",
    "    b = pd.merge(real_type, assembly_type, how='outer').assign(method='assembly-based').fillna(0)\n",
    "    c = pd.merge(real_type, binning_type, how='outer').assign(method='binning-based').fillna(0)\n",
    "    tval.append(\n",
    "        pd.concat([\n",
    "            a, b, c\n",
    "    ]).assign(sample = sample, coverage = coverage))\n",
    "\n",
    "    d = pd.merge(real_subtype, read_subtype, how='outer').assign(method='read-based').fillna(0)\n",
    "    e = pd.merge(real_subtype, assembly_subtype, how='outer').assign(method='assembly-based').fillna(0)\n",
    "    f = pd.merge(real_subtype, binning_subtype, how='outer').assign(method='binning-based').fillna(0)\n",
    "    sval.append(\n",
    "        pd.concat([\n",
    "            d, e, f\n",
    "    ]).assign(sample = sample, coverage = coverage))\n",
    "\n",
    "    for i,j in zip([a,b,c], ['read-based', 'assembly-based', 'binning-based']):\n",
    "        pearson = pearsonr(i.cnt, i.est)\n",
    "        spearman = spearmanr(i.cnt, i.est)\n",
    "        tstat.append([\n",
    "            cityblock(i.cnt, i.est), \n",
    "            euclidean(i.cnt, i.est), \n",
    "            pearson.statistic, pearson.pvalue, \n",
    "            spearman.statistic, spearman.pvalue,\n",
    "            j, sample, coverage, *pr(i)])\n",
    "\n",
    "    for i,j in zip([d,e,f], ['read-based', 'assembly-based', 'binning-based']):\n",
    "        pearson = pearsonr(i.cnt, i.est)\n",
    "        spearman = spearmanr(i.cnt, i.est)\n",
    "        sstat.append([\n",
    "            cityblock(i.cnt, i.est), \n",
    "            euclidean(i.cnt, i.est), \n",
    "            pearson.statistic, pearson.pvalue, \n",
    "            spearman.statistic, spearman.pvalue,\n",
    "            j, sample, coverage, *pr(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1b6e8e-1f4b-4ee1-adab-b2d8e1db8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(tval).sort_values(['method', 'sample', 'coverage']).to_csv('fig3/tval.tsv', sep='\\t', index=False)\n",
    "pd.concat(sval).sort_values(['method', 'sample', 'coverage']).to_csv('fig3/sval.tsv', sep='\\t', index=False)\n",
    "\n",
    "pd.DataFrame(tstat, columns = ['l1', 'l2', 'corr', 'p', 'corr.spearman', 'p.spearman', 'method', 'sample', 'coverage', 'precision', 'recall']).sort_values(['method', 'sample', 'coverage']).to_csv('fig3/tstat.tsv', sep='\\t', index=False)\n",
    "pd.DataFrame(sstat, columns = ['l1', 'l2', 'corr', 'p', 'corr.spearman', 'p.spearman', 'method', 'sample', 'coverage', 'precision', 'recall']).sort_values(['method', 'sample', 'coverage']).to_csv('fig3/sstat.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
